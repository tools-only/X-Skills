# Lora Peft

| Property | Value |
|----------|-------|
| **Name** | Lora Peft |
| **Repository** | [Jeffallan/claude-skills](https://raw.githubusercontent.com/Jeffallan/claude-skills/main/skills/fine-tuning-expert/references/lora-peft.md) (‚≠ê 216) |
| **Original Path** | `skills/fine-tuning-expert/references/lora-peft.md` |
| **Category** | productivity |
| **Subcategory** | optimization |
| **Tags** | productivity |
| **Created** | 2026-01-19 |
| **Updated** | 2026-01-29 |
| **File Hash** | `dcdc48e7c1701c2d...` |

## Description

ParameterEfficient FineTuning (PEFT) methods train only a small subset of model parameters while keeping the base model frozen. This dramatically reduces memory requirements and enables finetuning of large models on consumer hardware.

**Tags:** `productivity`

---

*This skill is maintained by [SkillFlow](https://github.com/tools-only/SkillFlow)*
*Source: [Jeffallan/claude-skills](https://raw.githubusercontent.com/Jeffallan/claude-skills/main/skills/fine-tuning-expert/references/lora-peft.md)*
