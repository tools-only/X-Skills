# Distributed Training - Comprehensive Guide

| Property | Value |
|----------|-------|
| **Name** | Distributed Training - Comprehensive Guide |
| **Repository** | [K-Dense-AI/claude-scientific-skills](https://raw.githubusercontent.com/K-Dense-AI/claude-scientific-skills/main/scientific-skills/pytorch-lightning/references/distributed_training.md) (ðŸ”¥ 7.7k) |
| **Original Path** | `scientific-skills/pytorch-lightning/references/distributed_training.md` |
| **Category** | productivity |
| **Subcategory** | optimization |
| **Tags** | productivity |
| **Created** | 2025-11-14 |
| **Updated** | 2025-11-14 |
| **File Hash** | `ccceed873c2498a3...` |

## Description

PyTorch Lightning provides several strategies for training large models efficiently across multiple GPUs, nodes, and machines. Choose the right strategy based on model size and hardware configuration.

**Tags:** `productivity`

---

*This skill is maintained by [SkillFlow](https://github.com/tools-only/SkillFlow)*
*Source: [K-Dense-AI/claude-scientific-skills](https://raw.githubusercontent.com/K-Dense-AI/claude-scientific-skills/main/scientific-skills/pytorch-lightning/references/distributed_training.md)*
