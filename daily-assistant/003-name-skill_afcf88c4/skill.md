---
name: tdd-process
description: "Strict test-driven development state machine with red-green-refactor cycles. Enforces test-first development, meaningful failures, minimum implementations, and full verification. Activates when user requests: 'use a TDD approach', 'start TDD', 'test-drive this'."
version: 1.0.0
---

**In Plan Mode: Plans should be test specifications, not implementation designs. Include key insights, architectural constraints, and suggestionsâ€”but never the full implementation of production code.**

# ğŸš¨ CRITICAL: TDD STATE MACHINE GOVERNANCE ğŸš¨

**EVERY SINGLE MESSAGE MUST START WITH YOUR CURRENT TDD STATE**

Format:
```
ğŸ”´ TDD: RED
ğŸŸ¢ TDD: GREEN
ğŸ”µ TDD: REFACTOR
âšª TDD: PLANNING
ğŸŸ¡ TDD: VERIFY
âš ï¸ TDD: BLOCKED
```

**NOT JUST THE FIRST MESSAGE. EVERY. SINGLE. MESSAGE.**

When you read a file â†’ prefix with TDD state
When you run tests â†’ prefix with TDD state
When you explain results â†’ prefix with TDD state
When you ask a question â†’ prefix with TDD state

Example:
```
âšª TDD: PLANNING
Writing test for negative price validation...

âšª TDD: PLANNING
Running npm test to see it fail...

âšª TDD: PLANNING
Test output shows: Expected CannotHaveNegativePrice error but received -50
Test fails correctly. Transitioning to RED.

ğŸ”´ TDD: RED
Test IS failing. Addressing what the error message demands...
```

**ğŸš¨ FAILURE TO ANNOUNCE TDD STATE = SEVERE VIOLATION ğŸš¨**

---

<meta_governance>
  ğŸš¨ STRICT STATE MACHINE GOVERNANCE ğŸš¨

  - CANNOT skip states or assume completion without evidence
  - MUST announce state on EVERY message
  - MUST validate post-conditions before transitioning

  **Before each response:** Verify your claimed state matches your tool call evidence.
  If mismatch: `ğŸ”¥ STATE VIOLATION DETECTED` â†’ announce correct state â†’ recover.

  **State announcement:** Every message starts with `ğŸ”´ TDD: RED` (or current state).
  Forgot prefix? Announce violation immediately, then continue.
</meta_governance>

<state_machine>
  <diagram>
```
                  user request
                       â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”Œâ”€â”€â”€â”€â”‚ PLANNING â”‚â”€â”€â”€â”€â”
            â”‚    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚
            â”‚          â”‚         â”‚
            â”‚  test fails        â”‚
            â”‚  correctly         â”‚
  unclear   â”‚          â†“         â”‚ blocker
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â””â”€â”€â”€â”€â”‚   RED    â”‚    â”‚
                 â”‚          â”‚    â”‚
                 â”‚ Test IS  â”‚    â”‚
                 â”‚ failing  â”‚    â”‚
                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚
                      â”‚          â”‚
              test    â”‚          â”‚
              passes  â”‚          â”‚
                      â†“          â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
                 â”‚  GREEN   â”‚    â”‚
                 â”‚          â”‚    â”‚
                 â”‚ Test IS  â”‚    â”‚
                 â”‚ passing  â”‚    â”‚
                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â”€â”€â”€â”€â”˜
                      â”‚
          refactoring â”‚
          needed      â”‚
                      â†“
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”Œâ”€â”€â”€â”€â”‚ REFACTOR â”‚
            â”‚    â”‚          â”‚
            â”‚    â”‚ Improve  â”‚
            â”‚    â”‚ design   â”‚
            â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚         â”‚
            â”‚    done â”‚
            â”‚         â”‚
            â”‚         â†“
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    â”‚  VERIFY  â”‚
            â”‚    â”‚          â”‚
            â”‚    â”‚ Run full â”‚
  fail      â”‚    â”‚ suite +  â”‚
            â”‚    â”‚ lint +   â”‚
            â””â”€â”€â”€â”€â”‚ build    â”‚
                 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                      â”‚
                 pass â”‚
                      â”‚
                      â†“
                 [COMPLETE]
```
  </diagram>

  <states>
    <state name="PLANNING">
      <prefix>âšª TDD: PLANNING</prefix>
      <purpose>Writing a failing test to prove requirement</purpose>

      <pre_conditions>
        âœ“ User has provided a task/requirement/bug report
        âœ“ No other TDD cycle in progress
      </pre_conditions>

      <actions>
        1. Analyze requirement/bug
        2. Ask clarifying questions if needed
        3. Determine what behavior needs testing
        4. Identify edge cases using writing-tests skill checklists (numbers, strings, collections, dates, null/undefined, typed property validation)
        5. Write test for specific behavior
        6. Run test (use Bash tool to execute test command)
        7. VERIFY test fails correctly
        8. Show exact failure message to user (copy/paste verbatim output)
        9. Justify why failure message proves test is correct
        10. If failure is "method doesn't exist" - implement empty/dummy method and re-run from step 6
        11. Repeat until you get a "meaningful" failure
        12. Improve the code to produce a more explicit error message. Does the test failure provide a precise reason for the failure, if not ask the user if they want to make it better.
        13. Transition to RED
      </actions>

      <post_conditions>
        âœ“ Test written and executed
        âœ“ Test FAILED correctly (red bar achieved)
        âœ“ Failure message shown to user verbatim
        âœ“ Failure reason justified (proves test is correct)
        âœ“ Failure is "meaningful" (not setup/syntax error)
      </post_conditions>

      <validation_before_transition>
        BEFORE transitioning to RED, announce:
        "Pre-transition validation:
        âœ“ Test written: [yes]
        âœ“ Test executed: [yes]
        âœ“ Test failed correctly: [yes]
        âœ“ Failure message shown: [yes - output above]
        âœ“ Meaningful failure: [yes - justification]

        Transitioning to RED - test is now failing for the right reason."
      </validation_before_transition>

      <transitions>
        - PLANNING â†’ RED (when test fails correctly - red milestone achieved)
        - PLANNING â†’ BLOCKED (when cannot write valid test)
      </transitions>
    </state>

    <state name="RED">
      <prefix>ğŸ”´ TDD: RED</prefix>
      <purpose>Test IS failing for the right reason. Implement ONLY what the error message demands.</purpose>

      ğŸš¨ CRITICAL: You are in RED state - test IS CURRENTLY FAILING. You MUST implement code and see test PASS, code COMPILE, code LINT before transitioning to GREEN.
      DO NOT transition to GREEN until you have:
      1. Implemented ONLY what the error message demands
      2. Executed the test with Bash tool
      3. Seen the SUCCESS output (green bar)
      4. Executed compile check and seen SUCCESS
      5. Executed lint check and seen PASS
      6. Shown all success outputs to the user

      <pre_conditions>
        âœ“ Test written and executed (from PLANNING)
        âœ“ Test IS FAILING correctly (red bar visible)
        âœ“ Failure message shown and justified
        âœ“ Failure is "meaningful" (not setup/syntax error)
      </pre_conditions>

      <actions>
        1. Read the error message - what does it literally ask for?
        2. ğŸš¨ MANDATORY SELF-CHECK - announce before implementing:
           "Minimal implementation check:
           - Error demands: [what the error literally says]
           - Could hardcoded value work? [yes/no]
           - If yes: [what hardcoded value]
           - If no: [why real logic is required]"

           Guidelines:
           - If test asserts `x === 5` â†’ return `5`
           - If test asserts `count === 0` â†’ return object with `count: 0`
           - If test asserts type â†’ return minimal stub of that type
           - Only add logic when tests FORCE you to (multiple cases, different inputs)
        3. Implement ONLY what that error message demands (hardcoded if possible)
        4. Do NOT anticipate future errors - address THIS error only
        5. Run test (use Bash tool to execute test command)
        6. VERIFY test PASSES (green bar)
        7. Show exact success message to user (copy/paste verbatim output)
        8. Run quick compilation check (e.g., tsc --noEmit, or project-specific compile command)
        9. Run lint on changed code
        10. If compile/lint fails: Fix issues and return to step 5 (re-run test)
        11. Show compile/lint success output to user
        12. Justify why implementation is minimum
        13. ONLY AFTER completing steps 5-12: Announce post-condition validation
        14. ONLY AFTER validation passes: Transition to GREEN

        ğŸš¨ YOU CANNOT TRANSITION TO GREEN UNTIL TEST PASSES, CODE COMPILES, AND CODE LINTS ğŸš¨
      </actions>

      <post_conditions>
        âœ“ Implemented ONLY what error message demanded
        âœ“ Test executed
        âœ“ Test PASSES (green bar - not red)
        âœ“ Success message shown to user verbatim
        âœ“ Code compiles (no compilation errors)
        âœ“ Code lints (no linting errors)
        âœ“ Compile/lint output shown to user
        âœ“ Implementation addresses ONLY what error message demanded (justified)
      </post_conditions>

      <validation_before_transition>
        ğŸš¨ BEFORE transitioning to GREEN, verify ALL with evidence from tool history:
        âœ“ Test PASSES (green bar) - show verbatim output
        âœ“ Code compiles - show output
        âœ“ Code lints - show output
        âœ“ Implementation addresses ONLY what error demanded - justify

        If ANY evidence missing: "âš ï¸ CANNOT TRANSITION - Missing: [what]" â†’ stay in RED.
      </validation_before_transition>

      <critical_rules>
        ğŸš¨ NEVER transition to GREEN without test PASS + compile SUCCESS + lint PASS
        ğŸš¨ IMPLEMENT ONLY WHAT THE ERROR MESSAGE DEMANDS - no anticipating future errors
        ğŸš¨ DON'T CHANGE TEST TO MATCH IMPLEMENTATION - fix the code, not the test
      </critical_rules>

      <transitions>
        - RED â†’ GREEN (when test PASSES, code COMPILES, code LINTS - green milestone achieved)
        - RED â†’ BLOCKED (when cannot make test pass or resolve compile/lint errors)
        - RED â†’ PLANNING (when test failure reveals requirement was misunderstood)
      </transitions>
    </state>

    <state name="GREEN">
      <prefix>ğŸŸ¢ TDD: GREEN</prefix>
      <purpose>Test IS passing for the right reason. Assess code quality and decide next step.</purpose>

      <pre_conditions>
        âœ“ Test exists and PASSES (from RED)
        âœ“ Test IS PASSING for the right reason (green bar visible)
        âœ“ Code compiles (no compilation errors)
        âœ“ Code lints (no linting errors)
        âœ“ Pass output was shown and implementation justified as minimum
      </pre_conditions>

      <actions>
        1. Review the implementation that made test pass
        2. Check code quality against object calisthenics
        3. Check for feature envy
        4. Check for dependency inversion opportunities
        5. Check naming conventions
        6. Decide: Does code need refactoring?
        7a. If YES refactoring needed â†’ Transition to REFACTOR
        7b. If NO refactoring needed â†’ Transition to VERIFY
      </actions>

      <post_conditions>
        âœ“ Test IS PASSING (green bar)
        âœ“ Code quality assessed
        âœ“ Decision made: refactor or verify
      </post_conditions>

      <validation_before_transition>
        BEFORE transitioning to REFACTOR or VERIFY, announce:
        "Post-condition validation:
        âœ“ Test IS PASSING: [yes - green bar visible]
        âœ“ Code quality assessed: [yes]
        âœ“ Decision: [REFACTOR needed / NO refactoring needed, go to VERIFY]

        All post-conditions satisfied. Transitioning to [REFACTOR/VERIFY]."

        IF any post-condition NOT satisfied:
        "âš ï¸ CANNOT TRANSITION - Post-condition failed: [which one]
        Staying in GREEN state to address: [issue]"
      </validation_before_transition>

      <critical_rules>
        ğŸš¨ GREEN state means test IS PASSING, code COMPILES, code LINTS - if any fail, you're back to RED
        ğŸš¨ NEVER skip code quality assessment
        ğŸš¨ NEVER transition if test is not passing
        ğŸš¨ NEVER transition if code doesn't compile or lint
        ğŸš¨ ALWAYS assess whether refactoring is needed
        ğŸš¨ Go to REFACTOR if improvements needed, VERIFY if code is already clean
      </critical_rules>

      <transitions>
        - GREEN â†’ REFACTOR (when refactoring needed - improvements identified)
        - GREEN â†’ VERIFY (when code quality satisfactory - no refactoring needed)
        - GREEN â†’ RED (if test starts failing - regression detected, need new failing test)
      </transitions>
    </state>

    <state name="REFACTOR">
      <prefix>ğŸ”µ TDD: REFACTOR</prefix>
      <purpose>Tests ARE passing. Improving code quality while maintaining green bar.</purpose>

      <pre_conditions>
        âœ“ Tests ARE PASSING (from GREEN)
        âœ“ Code compiles (no compilation errors)
        âœ“ Code lints (no linting errors)
        âœ“ Refactoring needs identified
        âœ“ Pass output was shown
      </pre_conditions>

      <actions>
        1. Analyze code for design improvements
        2. Check against project conventions
        3. Check naming conventions
        4. If improvements needed:
           a. Explain refactoring
           b. Apply refactoring
           c. Run test to verify behavior preserved
           d. Show test still passes
        5. Repeat until no more improvements
        6. Check tests for improvements opportunities - e.g. combine tests with it.each, check against project testing conventions (e.g. `/docs/conventions/testing.md`)
        7. Transition to VERIFY
      </actions>

      <post_conditions>
        âœ“ Code reviewed for quality
        âœ“ Object calisthenics applied
        âœ“ No feature envy
        âœ“ Dependencies inverted
        âœ“ Names are intention-revealing
        âœ“ Tests still pass after each refactor
        âœ“ Test output shown after each refactor
      </post_conditions>

      <validation_before_transition>
        BEFORE transitioning to VERIFY, announce:
        "Post-condition validation:
        âœ“ Object calisthenics: [applied/verified]
        âœ“ Feature envy: [none detected]
        âœ“ Dependencies: [properly inverted]
        âœ“ Naming: [intention-revealing]
        âœ“ Tests pass: [yes - output shown]

        All post-conditions satisfied. Transitioning to VERIFY."
      </validation_before_transition>

      <critical_rules>
        ğŸš¨ NEVER refactor without running tests after
        ğŸš¨ NEVER use generic names (data, utils, helpers)
        ğŸš¨ ALWAYS verify tests pass after refactor
      </critical_rules>

      <transitions>
        - REFACTOR â†’ VERIFY (when code quality satisfactory)
        - REFACTOR â†’ RED (if refactor broke test - write new test for edge case)
        - REFACTOR â†’ BLOCKED (if cannot refactor due to constraints)
      </transitions>
    </state>

    <state name="VERIFY">
      <prefix>ğŸŸ¡ TDD: VERIFY</prefix>
      <purpose>Tests ARE passing. Run full test suite + lint + build before claiming complete.</purpose>

      <pre_conditions>
        âœ“ Tests ARE PASSING (from GREEN or REFACTOR)
        âœ“ Code compiles (no compilation errors)
        âœ“ Code lints (no linting errors)
        âœ“ Either: Refactoring complete OR no refactoring needed
      </pre_conditions>

      <actions>
        1. Run full test suite (not just current test)
        2. Capture and show output
        3. Run lint
        4. Capture and show output
        5. Run build
        6. Capture and show output
        7. If ALL pass â†’ Transition to COMPLETE
        8. If ANY fail â†’ Transition to BLOCKED or RED
      </actions>

      <post_conditions>
        âœ“ Full test suite executed
        âœ“ All tests PASSED
        âœ“ Test output shown
        âœ“ Lint executed
        âœ“ Lint PASSED
        âœ“ Lint output shown
        âœ“ Build executed
        âœ“ Build SUCCEEDED
        âœ“ Build output shown
      </post_conditions>

      <validation_before_completion>
        BEFORE claiming COMPLETE, announce:
        "Final validation:
        âœ“ Full test suite: [X/X tests passed - output shown]
        âœ“ Lint: [passed - output shown]
        âœ“ Build: [succeeded - output shown]

        All validation passed. TDD cycle COMPLETE.

        Session Summary:
        - Tests written: [count]
        - Refactorings: [count]
        - Violations: [count]
        - Duration: [time]

        Next: Check if project defines a task workflow. If so, follow it to completion."

        IF any validation FAILED:
        "âš ï¸ VERIFICATION FAILED
        Failed check: [which one]
        Output: [failure message]

        Routing to: [RED/BLOCKED depending on issue]"
      </validation_before_completion>

      <critical_rules>
        ğŸš¨ NEVER claim complete without full test suite
        ğŸš¨ NEVER claim complete without lint passing
        ğŸš¨ NEVER claim complete without build passing
        ğŸš¨ ALWAYS show output of each verification
        ğŸš¨ NEVER skip verification steps
      </critical_rules>

      <transitions>
        - VERIFY â†’ COMPLETE (when all checks pass)
        - VERIFY â†’ RED (when tests fail - regression detected)
        - VERIFY â†’ REFACTOR (when lint fails - code quality issue)
        - VERIFY â†’ BLOCKED (when build fails - structural issue)
      </transitions>
    </state>

    <state name="BLOCKED">
      <prefix>âš ï¸ TDD: BLOCKED</prefix>
      <purpose>Handle situations where progress cannot continue</purpose>

      <pre_conditions>
        âœ“ Encountered issue preventing progress
        âœ“ Issue is not user error or misunderstanding
      </pre_conditions>

      <actions>
        1. Clearly explain blocking issue
        2. Explain which state you were in
        3. Explain what you were trying to do
        4. Explain why you cannot proceed
        5. Suggest possible resolutions
        6. STOP and wait for user guidance
      </actions>

      <post_conditions>
        âœ“ Blocker documented
        âœ“ Context preserved
        âœ“ Suggestions provided
        âœ“ Waiting for user
      </post_conditions>

      <critical_rules>
        ğŸš¨ ALWAYS stop and wait for user
      </critical_rules>

      <transitions>
        - BLOCKED â†’ [any state] (based on user guidance)
      </transitions>
    </state>

    <state name="VIOLATION_DETECTED">
      <prefix>ğŸ”¥ TDD: VIOLATION_DETECTED</prefix>
      <purpose>Handle state machine violations</purpose>

      <trigger>
        Self-detected violations:
        - Forgot state announcement
        - Skipped state
        - Failed to validate post-conditions
        - Claimed phase complete without evidence
        - Skipped test execution
        - Changed assertion when test failed
        - Changed test assertion to match implementation (instead of fixing implementation)
        - Implemented full solution when hardcoded value would satisfy error
        - Skipped mandatory self-check before implementing
      </trigger>

      <actions>
        1. IMMEDIATELY announce: "ğŸ”¥ STATE VIOLATION DETECTED"
        2. Explain which rule/state was violated
        3. Explain what you did wrong
        4. Announce correct current state
        5. Ask user permission to recover
        6. If approved, return to correct state
      </actions>

      <example>
        "ğŸ”¥ STATE VIOLATION DETECTED

        Violation: Forgot to announce state on previous message
        Current actual state: RED

        Recovering to correct state...

        ğŸ”´ TDD: RED
        [continue from here]"
      </example>
    </state>
  </states>
</state_machine>

<rules>
  <rule id="1" title="No Green Without Proof">
    If you didn't see green test output, tests didn't pass. Compilation/type-checking â‰  tests pass.
  </rule>

  <rule id="2" title="Show and justify failure before RED">
    In PLANNING: run test, show exact failure message, explain why it's the RIGHT failure.
    "Database migration failed" = setup issue, not meaningful failure.
  </rule>

  <rule id="3" title="Error message driven implementation">
    Implement ONLY what the error message literally demands.
    âŒ Method missing â†’ "not implemented" â†’ full solution
    âœ… Method missing â†’ return wrong value â†’ assertion failure â†’ hardcode correct value â†’ add test â†’ generalize

    Example techniques:
    - Return wrong value (null, 0, "") to get meaningful assertion failure
    - Hardcode expected value to pass, then add more tests to force real logic
    - Never jump from "not implemented" to full solution

    Concrete example:
    - Error: "Not implemented"
    - Test expects: componentCount: 0, linkCount: 0
    - âŒ WRONG: Implement real resume() with graph parsing logic
    - âœ… RIGHT: Return hardcoded stub `{ componentCount: 0, linkCount: 0 }`
    - Then: Add more tests to force real implementation
  </rule>

  <rule id="4" title="Predict user response">
    When asking questions, predict the answer: "Should I fix or skip? Given TDD context, you'll want option 1."
  </rule>

  <rule id="5" title="Green = test + lint + build">
    Never claim GREEN without all three passing and showing output.
  </rule>

  <rule id="6" title="Add observability">
    Add debug data (report objects, logging) so test failures are diagnosable.
  </rule>

  <rule id="7" title="Never change assertions to pass">
    Test fails? Fix IMPLEMENTATION, not test. Changing assertion = VIOLATION_DETECTED.
    If test is actually wrong: revert, fix test, re-implement.
  </rule>

  <rule id="8" title="Fail fast, no silent fallbacks">
    âŒ `value ?? backup ?? 'Unknown'` â†’ âœ… `if (!value) throw Error('Expected value')`
  </rule>

  <rule id="9" title="No guessing">
    Never "probably". Add diagnostics, get evidence, report facts.
  </rule>

  <rule id="10" title="Minimal assertions">
    `expect(x).toBe('exact')` subsumes `toBeDefined()` and length checks. One strong assertion, not defensive scaffolding.
  </rule>

  <meta_rule>
    ALL rules enforced via state machine post-conditions.
    Skip validation â†’ VIOLATION_DETECTED
  </meta_rule>

  <critical_reminders>
    ğŸš¨ EVERY message: state announcement
    ğŸš¨ NEVER skip transitions or claim pass/fail without output
    ğŸš¨ ALWAYS justify: does this address ONLY what error message demanded?
    ğŸš¨ NEVER change assertions to make tests pass
    ğŸš¨ NEVER guess - find evidence
  </critical_reminders>
</rules>
