# Humaneval

| Property | Value |
|----------|-------|
| **Name** | Humaneval |
| **Repository** | [greynewell/mcpbr](https://raw.githubusercontent.com/greynewell/mcpbr/main/docs/benchmarks/humaneval.md) (‚≠ê 20) |
| **Original Path** | `docs/benchmarks/humaneval.md` |
| **Category** | development |
| **Subcategory** | coding |
| **Tags** | development |
| **Created** | 2026-02-01 |
| **Updated** | 2026-02-06 |
| **File Hash** | `a8f592af4e0afd0c...` |

## Description

HumanEval evaluates AI agents on 164 Python programming problems from OpenAI, testing code generation from function signatures and docstrings with unit test verification.

**Tags:** `development`

---

*This skill is maintained by [SkillFlow](https://github.com/tools-only/SkillFlow)*
*Source: [greynewell/mcpbr](https://raw.githubusercontent.com/greynewell/mcpbr/main/docs/benchmarks/humaneval.md)*
