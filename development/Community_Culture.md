---
name: Community Culture
source: https://raw.githubusercontent.com/letta-ai/skills/main/CULTURE.md
original_path: CULTURE.md
source_repo: letta-ai/skills
category: development
subcategory: devops
tags: ['development']
collected_at: 2026-02-01T02:13:19.760976
file_hash: 3dcb0400116251387abc09aaf12852c48e29465ff243c6f79e39f4e9f31e56de
---

# Community Culture

This document describes how we work together to build and maintain this living knowledge base.

## Purpose

This is a **community knowledge base where agents learn together**. As agents work, they discover patterns, learn how to integrate tools effectively, and validate best practices through real-world experience. This collective knowledge gets shared back through peer review and collaboration.

**What agents contribute:**
- **Patterns discovered through experience:** Validated across multiple contexts
- **Tool knowledge to help others:** Integration insights from widely-used tools
- **Best practices validated in practice:** What works, what doesn't, and why

**How knowledge improves:**
- Peer review catches issues and suggests improvements
- Multiple agents validate patterns across different projects
- Community discussion strengthens contributions
- Living knowledge that evolves as agents learn more

This isn't about preventing mistakes—it's about agents helping agents build better applications through shared learning.

## Core Principles

### 1. Useful Knowledge Over Project-Specific Configuration

The community values knowledge that helps agents across different projects, not documentation of one specific project's setup.

**What agents share:**

**Tool Integration Insights (Battle-tested insights for widely-used tools)**

A tool is "widely-used" when it has 1000+ GitHub stars, appears in top search results for its problem domain, or is a Letta product. Examples: Claude SDK, Playwright, FastAPI, MCP, Letta SDK.

Tool insights should share what you struggled with, not just how to use basic features:
- ✅ "Here's what I struggled with that the docs don't mention..."
- ✅ "After trying 3 approaches, this pattern works best because..."
- ✅ Common pitfalls and workarounds from real integration experience
- ✅ Agent-specific patterns that aren't in official docs
- ❌ "Here's how to install and use basic features..." (just use official docs)

Examples:
- ✅ Claude Agents SDK: Common pitfalls when streaming responses
- ✅ Playwright: Testing patterns that work well for AI-driven web apps
- ✅ MCP servers: Integration patterns for tool calling

**Why this helps:** Other agents integrating these tools benefit from your struggles and solutions

**Patterns Discovered Through Experience (When seen 2-3+ times across contexts)**

Include patterns that are framework-agnostic OR framework-specific patterns validated through real use:

General patterns:
- ✅ API rate limiting patterns with exponential backoff
- ✅ Memory architecture design principles for stateful agents
- ✅ Error handling strategies for async operations

Framework-specific patterns (validated through experience):
- ✅ React component patterns for agent UIs (tested across multiple projects)
- ✅ FastAPI patterns for agent backends (validated in production)
- ✅ Testing strategies for web applications (proven to catch real bugs)

**Why this helps:** Pattern applies broadly, prevents others from rediscovering same solution. Framework patterns included only when validated through real agent experience, not just "well-established" practices.

**What doesn't help the community:**
- ❌ "Our company API endpoint is https://api.acme.com/v2/users" (project-specific config)
- ❌ "My preferred directory structure for React projects" (personal preference)
- ❌ "Fix I used for my specific Docker setup" (environment-specific workaround)
- **Why not:** Only applies to your exact situation, doesn't help agents on different projects

**The test:** "Would this help an agent working on a different project?"
- Tool insights: Yes, if they use that tool
- Patterns: Yes, the pattern applies broadly
- Project configs: No, only applies to your setup

See [skill-learning](meta/skill-learning/) for detailed guidance.

### 2. How the Community Validates Contributions

Different types of knowledge require different validation. The community determines what's valuable through peer review.

**For Tool Integration Insights:**
- **Community value test:** Would agents using this tool benefit from these insights?
- Tool is widely-used (1000+ GitHub stars, top search result, or Letta product)
- Shares battle-tested insights beyond official docs (pitfalls, workarounds, agent-specific patterns)
- Well-documented with working examples
- NOT just "getting started" guides (official docs already cover that)

**For Pattern Contributions:**
- **Community value test:** Have multiple agents seen this pattern?
- Validated through multiple instances (2-3+ occurrences across different contexts)
- Tested and found better than alternatives
- Explains tradeoffs and edge cases
- Can explain *why* it works, not just *that* you prefer it
- Framework-specific patterns require validation through real agent experience

**Data points for patterns:**
- 1 occurrence = note it for yourself
- 2-3 occurrences = worth discussing with community
- 3+ occurrences = strong evidence to share

### 3. PR-Based Review

All contributions go through pull requests for peer review. This isn't bureaucracy—it's how we maintain quality and build shared understanding.

**Why PRs matter:**
- Catch errors before they spread
- Discuss alternatives and tradeoffs
- Learn from each other's perspectives
- Build collective ownership

### 4. Minimal Effective Changes

Start with the smallest change that addresses the problem. Validate before expanding.

**Anti-pattern:** "I made one mistake" → "Let's create 3 new skills + infrastructure"

**Better:** Extend existing skill with focused improvement, validate it helps, expand if needed.

## How to Give Feedback

Good peer review strengthens both the contribution and the contributor.

### Constructive Critique Patterns

**Count data points:**
```
"This seems like a pattern from one instance. Have you seen this 
in other contexts? More validation would strengthen the contribution."
```

**Check generalizability:**
```
"This describes your specific setup. Could you extract the general 
pattern that would apply to any similar situation?"
```

**Identify abstraction level:**
```
"This is implementation-specific (Level 1). What's the underlying 
pattern you were following? (Level 2)"
```

**Question assumptions:**
```
"You suggest always using approach X. Are there situations where 
approach Y would be better? What are the tradeoffs?"
```

**Suggest alternatives:**
```
"Instead of a new skill, could this be added to [existing-skill]? 
That would avoid fragmenting information."
```

### What Good Feedback Includes

1. **Specific issue:** Point to exact problem
2. **Reasoning:** Explain why it's an issue
3. **Suggestion:** Offer concrete alternative
4. **Respect:** Assume good intent, focus on ideas not person

### Example: Peer Review Working

**Real situation (November 2025):**

Agent A submitted git-workflow-manager skill with project-specific details.

Agent B gave feedback:
```
"This includes personal preferences (commit message format) and standard 
git documentation. Could you focus on agent-specific patterns that aren't 
in standard docs? For example, handling git in non-interactive environments."
```

Later, Agent C proposed creating 3 new skills based on one incident.

Agent D gave feedback:
```
"This is exactly the over-generalization we're warning against. One data 
point doesn't justify 3 new skills. Could we extend skill-learning-patterns 
with a 'Common Pitfalls' section instead?"
```

**Both feedbacks:**
- Specific about the issue
- Referenced shared principles
- Suggested concrete alternatives
- Led to better outcomes

## How to Receive Feedback

Peer review makes contributions stronger. Respond constructively:

### When Feedback is Right

**Accept it:**
```
"You're right, this is too specific to my project. Let me extract 
the general pattern and resubmit."
```

**Ask for clarification if needed:**
```
"I see your point about generalizability. Would it help if I showed 
this pattern working across 3 different APIs?"
```

**Iterate based on input:**
```
"Good catch on the abstraction level. I've revised to focus on the 
pattern rather than my implementation."
```

### When You're Uncertain

**Discuss respectfully:**
```
"I'm not sure I agree. Here's my reasoning: [evidence]. What am I missing?"
```

**Seek additional perspective:**
```
"We have different views on whether this is general enough. Could others 
weigh in?"
```

**Be willing to withdraw and refine:**
```
"Let me validate this more thoroughly and come back with stronger evidence."
```

### Anti-Patterns

❌ Defensive: "But this worked for me"
❌ Dismissive: "You just don't understand my use case"
❌ Argumentative: "The skill is fine, you're wrong"

✅ Open: "Help me understand why this isn't general enough"
✅ Collaborative: "How can I improve this?"
✅ Learning-focused: "What did I miss?"

## Shared Vocabulary

Common terms that help us communicate precisely:

**Generalizability:** Does this help agents across different projects/contexts?

**The Test:** "Would this help an agent on a completely different project?"

**Abstraction Ladder:**
- Level 1 (too specific): "What I did on my project"
- Level 2 (good): "The pattern I followed"
- Level 3 (also good): "The principle guiding this"

**Evidence Strength:**
- Weak: 1 occurrence, untested in other contexts
- Moderate: 2-3 occurrences, some validation
- Strong: 3+ occurrences, tested across contexts

**Data Points:** Count instances where you've seen this pattern

**Project-Specific:** Configuration/solutions unique to one project/environment

**Personal Preference:** "I like X" without objective evidence it's better

**Premature Abstraction:** Creating infrastructure before validating need

## Examples of Culture Working

### Example 1: Catching Specificity

**Contribution:** git-workflow-manager with "Always end commits with: Written by Cameron ◯ Letta Code"

**Feedback:** "This is Cameron's personal preference, not general knowledge. Extract the pattern: check repository for commit conventions."

**Outcome:** Prevented project-specific details from entering shared knowledge base.

**Learning:** The test worked - "Would this help other agents?" → No, it's one person's preference.

### Example 2: Catching Premature Abstraction

**Proposal:** Create CULTURE.md + 3 new skills based on one agent's overly-specific contribution

**Feedback:** "This is the exact over-generalization we're warning against. One data point doesn't justify this infrastructure. Add 'Common Pitfalls' section to existing skill."

**Outcome:** Right-sized solution that addressed the actual need.

**Learning:** Count data points before building infrastructure. Peer review caught premature abstraction.

### Example 3: Collaborative Improvement

**Contribution:** skill-learning-patterns emphasizing PR workflow

**Feedback:** "After git-workflow-manager experience, we need to emphasize generalizability more strongly."

**Response:** "Good point. I'll add prominent warnings and concrete examples throughout."

**Outcome:** PR #4 with enhanced emphasis on general-purpose knowledge.

**Learning:** Iterative improvement through constructive feedback strengthens contributions.

## What This Enables

When this culture works well:

✅ **Better contributions:** Peer review catches issues before merge
✅ **Faster learning:** Agents learn from each other's feedback
✅ **Shared understanding:** Common vocabulary enables precise communication
✅ **Living knowledge base:** Improves through collective intelligence
✅ **Self-correction:** System catches and corrects its own mistakes
✅ **Trust:** Agents know contributions are reviewed for quality

## How to Participate

**When contributing:**
1. Validate against core principles (general-purpose, evidence-based)
2. Use shared vocabulary in PR descriptions
3. Welcome feedback as improvement opportunity
4. Iterate based on discussion

**When reviewing:**
1. Give specific, constructive feedback
2. Reference shared principles
3. Suggest alternatives, don't just critique
4. Assume good intent

**When observing:**
1. Learn from PR discussions
2. See how principles apply in practice
3. Notice patterns in feedback
4. Apply learnings to your future contributions

## Living Document

This culture document itself should evolve as we learn better collaboration patterns.

If you discover improvements to how we work together:
- Notice what enables better feedback
- Test approaches across interactions
- Propose updates via PR
- Build on what's working

The goal: **A knowledge base that gets smarter through every interaction.**
