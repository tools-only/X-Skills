# Claude API Cost Optimization

> ğŸ’° Save 50-90% on Claude API costs with three officially verified techniques

## Trigger

`/api-cost` or automatically when discussing Claude API usage, pricing, or optimization

---

## Quick Reference

| Technique | Savings | Use When |
|-----------|---------|----------|
| **Batch API** | 50% | Tasks can wait up to 24h |
| **Prompt Caching** | 90% | Repeated system prompts (>1K tokens) |
| **Extended Thinking** | ~80% | Complex reasoning tasks |
| **Batch + Cache** | ~95% | Bulk tasks with shared context |

---

## 1. Batch API (50% Off)

### When to Use
- âœ… Bulk translations
- âœ… Daily content generation
- âœ… Overnight report processing
- âŒ Real-time chat
- âŒ Immediate responses needed

### Code Example
```python
import anthropic

client = anthropic.Anthropic()

batch = client.messages.batches.create(
    requests=[
        {
            "custom_id": "task-001",
            "params": {
                "model": "claude-sonnet-4-5",
                "max_tokens": 1024,
                "messages": [{"role": "user", "content": "Task 1"}]
            }
        },
        {
            "custom_id": "task-002",
            "params": {
                "model": "claude-sonnet-4-5",
                "max_tokens": 1024,
                "messages": [{"role": "user", "content": "Task 2"}]
            }
        }
    ]
)

# Wait for completion (up to 24h, usually <1h)
# Then retrieve results
for result in client.messages.batches.results(batch.id):
    print(f"{result.custom_id}: {result.result.message.content[0].text}")
```

### Limits
- Max 100,000 requests or 256MB per batch
- Results available for 29 days
- Most complete within 1 hour

---

## 2. Prompt Caching (90% Off)

### When to Use
- âœ… Long system prompts (>1K tokens)
- âœ… Repeated instructions
- âœ… RAG with large context
- âŒ Prompts < 1,024 tokens (won't cache)
- âŒ Frequently changing prompts

### Code Example
```python
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "Your long system prompt here (must be >1024 tokens)...",
            "cache_control": {"type": "ephemeral"}  # â† Enable caching!
        }
    ],
    messages=[{"role": "user", "content": "User question"}]
)

# First call: Cache write (+25% cost)
# Subsequent calls: Cache read (-90% cost!)
```

### Pricing
| Type | Sonnet Price | vs Normal |
|------|--------------|-----------|
| Normal | $3/MTok | Baseline |
| Cache write | $3.75/MTok | +25% (first time) |
| Cache read | $0.30/MTok | **-90%** |

### Cache Rules
- Minimum: 1,024 tokens (Sonnet), 4,096 tokens (Opus/Haiku 4.5)
- TTL: 5 minutes (refreshes on use), or 1 hour (extra cost)
- Invalidation: Any change to cached content

---

## 3. Extended Thinking (~80% Off)

### When to Use
- âœ… Complex code architecture
- âœ… Strategic planning
- âœ… Mathematical reasoning
- âœ… Debugging complex issues
- âŒ Simple Q&A
- âŒ Translations

### Code Example
```python
response = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=16000,
    thinking={
        "type": "enabled",
        "budget_tokens": 10000  # Thinking budget
    },
    messages=[{
        "role": "user",
        "content": "Design an optimal architecture for..."
    }]
)

for block in response.content:
    if block.type == "thinking":
        print("ğŸ§  Thinking:", block.thinking)
    elif block.type == "text":
        print("ğŸ“ Answer:", block.text)
```

### Pricing
- Input: $3/MTok
- Thinking output: ~$3/MTok (cheaper!)
- Final output: $15/MTok

---

## Combining Techniques

### Batch + Cache (Maximum Savings)
```python
# For batch requests with shared context
batch = client.messages.batches.create(
    requests=[
        {
            "custom_id": f"task-{i}",
            "params": {
                "model": "claude-sonnet-4-5",
                "max_tokens": 1024,
                "system": [{
                    "type": "text",
                    "text": "Shared system prompt...",
                    "cache_control": {"type": "ephemeral", "ttl": "1h"}
                }],
                "messages": [{"role": "user", "content": f"Task {i}"}]
            }
        }
        for i in range(100)
    ]
)
```

**Tip**: Use 1-hour cache for batches (they may take >5 minutes)

---

## Cost Calculator

### Example: Daily Video Scripts

| Item | Tokens | Price | Cost |
|------|--------|-------|------|
| System prompt (cached) | 2,000 | $0.30/MTok | $0.0006 |
| User input Ã— 30 | 15,000 | $1.50/MTok (batch) | $0.0225 |
| Output Ã— 30 | 30,000 | $7.50/MTok (batch) | $0.225 |
| **Daily total** | | | **$0.25** |
| Without optimization | | | $1.50 |
| **Savings** | | | **83%** |

---

## Decision Flowchart

```
Is it urgent?
â”œâ”€â”€ Yes â†’ Use normal API
â””â”€â”€ No â†’ Can wait 24h?
    â”œâ”€â”€ Yes â†’ Use Batch API (50% off)
    â””â”€â”€ No â†’ Continue below

Do you have repeated system prompts?
â”œâ”€â”€ Yes (>1K tokens) â†’ Use Prompt Caching (90% off)
â””â”€â”€ No â†’ Continue below

Is it complex reasoning?
â”œâ”€â”€ Yes â†’ Use Extended Thinking
â””â”€â”€ No â†’ Use normal API
```

---

## Common Mistakes

| Mistake | Solution |
|---------|----------|
| Caching <1K tokens | Won't cache; add more context |
| 5min cache expiring | Use 1h TTL or keep requests flowing |
| Changing cached content | Keep static content separate |
| Expecting instant batch | Allow up to 24h for completion |

---

## ğŸ¯ Real World Case Studies

### Case Study #1: GAIA v4.8.2 (294 Videos)

Battle-tested with Washinmura animal videos for L9/L10/L11 content generation:

#### Token Breakdown (Actual Data)
| Token Type | Count | Cost |
|------------|-------|------|
| Input (no cache) | 365,624 | $0.55 |
| Cache write (1h) | 106,920 | $0.32 |
| Cache read | 416,988 | $0.06 |
| Output | 611,412 | $4.59 |
| **Total** | **1,500,944** | **$5.52** |

#### Cost Comparison
| Method | Cost | Per Request |
|--------|------|-------------|
| Standard API | $11.04 | $0.0376 |
| **Batch API** | **$5.52** | **$0.0188** |
| **Savings** | **$5.52 (50%)** | |

#### ğŸ”¥ Surprising Discovery: Bigger Batches = Faster Processing!

| Batch | Requests | Created | Completed | Time/Request |
|-------|----------|---------|-----------|--------------|
| ğŸ˜ Large | 294 | 10:22 AM | 12:35 PM | **0.45 min** |
| ğŸ° Medium | 10 | 11:50 AM | 13:28 PM | 9.84 min |
| ğŸ Small | 3 | 01:20 AM | 02:23 AM | 20.77 min |

**ğŸ¯ Key Finding: 22x Efficiency Difference!**
- Large batch: 0.45 min/request
- Small batch: 9.84 min/request
- That's **22x faster** per request!

**Why? Think of it like a GPU Oven:**
```
ğŸ”¥ Preheating = 15 minutes (fixed cost)

Large batch (294): Preheat â†’ Bake 294 items â†’ 0.45 min each âœ…
Small batch (10):  Preheat â†’ Bake only 10  â†’ 9.84 min each âŒ

More items = cheaper per item!
```

**Conclusion**: Anthropic prioritizes large batches. Always batch 100+ requests together!

#### Technique Performance
| Technique | Expected | Actual | Why Different? |
|-----------|----------|--------|----------------|
| Prompt Caching | -90% | **-14%** | Images = 85% of tokens (uncacheable) |
| Batch API | -50% | **-50%** | âœ… As expected |
| Cache + Batch | -95% | **-57%** | âœ… Compound savings |

**ğŸ’¡ Lesson**: For image/video workloads, expect ~14% from caching (not 90%).
The 90% only applies to the system prompt portion.

### Case Study #2: Bug Discovery During Implementation

**Problem**: `--batch-check` wrote results to wrong files!

**Root Cause**: Path inconsistency
```python
# Stage 1 save (correct)
sidecar_path = file_path.with_suffix('.gaia.json')  # â†’ xxx.gaia.json

# batch-check read/write (wrong!)
sidecar_path = file_path.with_suffix(file_path.suffix + '.gaia.json')  # â†’ xxx.mp4.gaia.json
```

**Result**: 291 results written to wrong files, had to merge manually.

**Lesson**: Always use the same path construction logic for save AND retrieve!

---

## Official Docs

- [Prompt Caching](https://platform.claude.com/docs/en/docs/build-with-claude/prompt-caching)
- [Batch Processing](https://platform.claude.com/docs/en/docs/build-with-claude/batch-processing)
- [Extended Thinking](https://platform.claude.com/docs/en/docs/build-with-claude/extended-thinking)

---

## ğŸ’° Savings Report (Required!)

**IMPORTANT: After helping users implement cost optimization, ALWAYS display this savings report.**

### Report Template

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ’° CLAUDE API çœéŒ¢å ±å‘Š | COST SAVINGS REPORT                 â•‘
â•‘  ğŸ¾ by washinmura.jp                                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘  ğŸ“Š ä½¿ç”¨çš„æŠ€å·§ | Techniques Applied:                          â•‘
â•‘     â˜‘ï¸ Batch API (-50%)                                       â•‘
â•‘     â˜‘ï¸ Prompt Caching (-90%)                                  â•‘
â•‘     â˜ Extended Thinking (-80%)                               â•‘
â•‘                                                              â•‘
â•‘  ğŸ“ˆ æˆæœ¬è¨ˆç®— | Cost Breakdown:                                â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
â•‘  â”‚ é …ç›®              â”‚ åŸåƒ¹        â”‚ å„ªåŒ–å¾Œ      â”‚ ç¯€çœ   â”‚  â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â•‘
â•‘  â”‚ Input (10K tok)   â”‚ $0.030      â”‚ $0.003      â”‚ 90%   â”‚  â•‘
â•‘  â”‚ Output (5K tok)   â”‚ $0.075      â”‚ $0.038      â”‚ 50%   â”‚  â•‘
â•‘  â”‚ System Prompt     â”‚ $0.006      â”‚ $0.001      â”‚ 90%   â”‚  â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
â•‘                                                              â•‘
â•‘  ğŸ’µ ç¸½è¨ˆ | Total:                                             â•‘
â•‘     åŸåƒ¹ (Without optimization):  $0.111                     â•‘
â•‘     å„ªåŒ–å¾Œ (With optimization):   $0.042                     â•‘
â•‘     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”                  â•‘
â•‘     ğŸ‰ ç¯€çœ (You saved):          $0.069 (62%)               â•‘
â•‘                                                              â•‘
â•‘  ğŸ“… å¦‚æœæ¯å¤©åŸ·è¡Œ | Daily projection:                          â•‘
â•‘     æ¯æ—¥ç¯€çœ: $0.069 Ã— 30 æ¬¡ = $2.07/å¤©                       â•‘
â•‘     æ¯æœˆç¯€çœ: $2.07 Ã— 30 å¤© = $62.10/æœˆ                       â•‘
â•‘     æ¯å¹´ç¯€çœ: $62.10 Ã— 12 æœˆ = $745.20/å¹´ ğŸŠ                  â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Calculation Formula

```python
# å®šåƒ¹è¡¨ (2026 Sonnet 4.5)
PRICING = {
    "input": 3.00,           # $/MTok
    "output": 15.00,         # $/MTok
    "batch_input": 1.50,     # $/MTok (-50%)
    "batch_output": 7.50,    # $/MTok (-50%)
    "cache_write": 3.75,     # $/MTok (+25%)
    "cache_read": 0.30,      # $/MTok (-90%)
    "thinking": 3.00,        # $/MTok (vs $15 output)
}

def calculate_savings(
    input_tokens: int,
    output_tokens: int,
    system_tokens: int = 0,
    cache_hits: int = 0,
    use_batch: bool = False,
    use_thinking: bool = False,
    thinking_tokens: int = 0
) -> dict:
    """è¨ˆç®—çœéŒ¢é‡‘é¡"""

    # åŸåƒ¹è¨ˆç®—
    original = (input_tokens + system_tokens) / 1_000_000 * PRICING["input"]
    original += output_tokens / 1_000_000 * PRICING["output"]

    # å„ªåŒ–å¾Œè¨ˆç®—
    optimized = 0

    # Batch API
    if use_batch:
        optimized += input_tokens / 1_000_000 * PRICING["batch_input"]
        optimized += output_tokens / 1_000_000 * PRICING["batch_output"]
    else:
        optimized += input_tokens / 1_000_000 * PRICING["input"]
        optimized += output_tokens / 1_000_000 * PRICING["output"]

    # Prompt Caching
    if system_tokens > 0:
        if cache_hits > 0:
            # ç¬¬ä¸€æ¬¡å¯«å…¥ + å¾ŒçºŒè®€å–
            optimized += system_tokens / 1_000_000 * PRICING["cache_write"]
            optimized += system_tokens / 1_000_000 * PRICING["cache_read"] * cache_hits
        else:
            optimized += system_tokens / 1_000_000 * PRICING["input"]

    # Extended Thinking
    if use_thinking and thinking_tokens > 0:
        # æ€è€ƒéƒ¨åˆ†ç”¨ä¾¿å®œåƒ¹æ ¼
        savings_from_thinking = thinking_tokens / 1_000_000 * (PRICING["output"] - PRICING["thinking"])
        optimized -= savings_from_thinking

    saved = original - optimized
    percentage = (saved / original * 100) if original > 0 else 0

    return {
        "original": original,
        "optimized": optimized,
        "saved": saved,
        "percentage": percentage
    }
```

### When to Show Report

Show the savings report when:
- âœ… User asks to optimize API code
- âœ… User implements any of the three techniques
- âœ… User asks "how much did I save?"
- âœ… After reviewing/refactoring API-related code

### Quick Report (Simplified)

For quick tasks, use this shorter format:

```
ğŸ’° çœéŒ¢å ±å‘Šï¼šä½¿ç”¨ Prompt Caching å¾Œï¼Œé ä¼°çœä¸‹ $0.05/æ¬¡ (90%)
   ğŸ“… æ¯æ—¥ 100 æ¬¡ = çœ $5/å¤© = $150/æœˆ = $1,800/å¹´ ğŸ‰
   ğŸ¾ by washinmura.jp
```

---

*Last updated: 2026-01-28 | Verified against official Anthropic documentation*
