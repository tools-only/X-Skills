# Skill

| Property | Value |
|----------|-------|
| **Name** | Skill |
| **Repository** | [davila7/claude-code-templates](https://raw.githubusercontent.com/davila7/claude-code-templates/main/cli-tool/components/skills/ai-research/inference-serving-llama-cpp/SKILL.md) (ðŸ”¥ 19.4k) |
| **Original Path** | `cli-tool/components/skills/ai-research/inference-serving-llama-cpp/SKILL.md` |
| **Category** | development |
| **Subcategory** | devops |
| **Tags** | Inference Serving, Llama.cpp, CPU Inference, Apple Silicon, Edge Deployment |
| **Created** | 2026-01-08 |
| **Updated** | 2026-01-08 |
| **File Hash** | `d74caa45483f5b3c...` |

## Description

Pure C/C++ LLM inference with minimal dependencies, optimized for CPUs and nonNVIDIA hardware.

**Tags:** `Inference Serving` `Llama.cpp` `CPU Inference` `Apple Silicon` `Edge Deployment`

---

*This skill is maintained by [SkillFlow](https://github.com/tools-only/SkillFlow)*
*Source: [davila7/claude-code-templates](https://raw.githubusercontent.com/davila7/claude-code-templates/main/cli-tool/components/skills/ai-research/inference-serving-llama-cpp/SKILL.md)*
