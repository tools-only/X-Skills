# Skill

| Property | Value |
|----------|-------|
| **Name** | Skill |
| **Repository** | [davila7/claude-code-templates](https://raw.githubusercontent.com/davila7/claude-code-templates/main/cli-tool/components/skills/ai-research/inference-serving-tensorrt-llm/SKILL.md) (ðŸ”¥ 19.4k) |
| **Original Path** | `cli-tool/components/skills/ai-research/inference-serving-tensorrt-llm/SKILL.md` |
| **Category** | development |
| **Subcategory** | devops |
| **Tags** | Inference Serving, TensorRT-LLM, NVIDIA, Inference Optimization, High Throughput |
| **Created** | 2026-01-08 |
| **Updated** | 2026-01-08 |
| **File Hash** | `13f8e09b3e05b216...` |

## Description

NVIDIA's opensource library for optimizing LLM inference with stateoftheart performance on NVIDIA GPUs.

**Tags:** `Inference Serving` `TensorRT-LLM` `NVIDIA` `Inference Optimization` `High Throughput`

---

*This skill is maintained by [SkillFlow](https://github.com/tools-only/SkillFlow)*
*Source: [davila7/claude-code-templates](https://raw.githubusercontent.com/davila7/claude-code-templates/main/cli-tool/components/skills/ai-research/inference-serving-tensorrt-llm/SKILL.md)*
