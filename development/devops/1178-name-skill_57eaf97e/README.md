# Skill

| Property | Value |
|----------|-------|
| **Name** | Skill |
| **Repository** | [davila7/claude-code-templates](https://raw.githubusercontent.com/davila7/claude-code-templates/main/cli-tool/components/skills/ai-research/inference-serving-vllm/SKILL.md) (ðŸ”¥ 19.4k) |
| **Original Path** | `cli-tool/components/skills/ai-research/inference-serving-vllm/SKILL.md` |
| **Category** | development |
| **Subcategory** | devops |
| **Tags** | vLLM, Inference Serving, PagedAttention, Continuous Batching, High Throughput |
| **Created** | 2026-01-08 |
| **Updated** | 2026-01-08 |
| **File Hash** | `57eaf97e982fe758...` |

## Description

vLLM achieves 24x higher throughput than standard transformers through PagedAttention (blockbased KV cache) and continuous batching (mixing prefill/decode requests).

**Tags:** `vLLM` `Inference Serving` `PagedAttention` `Continuous Batching` `High Throughput`

---

*This skill is maintained by [SkillFlow](https://github.com/tools-only/SkillFlow)*
*Source: [davila7/claude-code-templates](https://raw.githubusercontent.com/davila7/claude-code-templates/main/cli-tool/components/skills/ai-research/inference-serving-vllm/SKILL.md)*
